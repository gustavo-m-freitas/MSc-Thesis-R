---
title: "Volatility Models"
author: "Gustavo Freitas"
subtitle: "Time Series"
output: 
  html_notebook: default
  pdf_document: 
    latex_engine: xelatex
  github_document: default
---

### **Introduction:** 

This document provides a comprehensive analysis of volatility forecasting models applied to the FTSE-100 index. It highlights the construction of a robust dataset and the implementation of various models, including HAR, ARIMA, SMA, EMA, EWMA, and ARCH/GARCH, to predict realized volatility. By evaluating these models with key metrics such as MAE, RMSE, and R², the study demonstrates their practical application and comparative performance in volatility forecasting.


```{r setup, include=FALSE}
# Load required packages
library(dplyr)
library(tidyr)
library(readr)
library(purrr)
library(slider)
library(ggplot2)
library(forecast)
```

## **Construction and Transformation of the FTSE Dataset**
The ftse dataset was constructed through a multi-step process involving data collection, cleaning, transformation, and volatility computation. Initially, FTSE-100 index data from multiple yearly files (2000-2009) was consolidated into a single dataset. After removing problematic dates and verifying data integrity, the dataset was aggregated into 5-minute intervals, and log returns were computed as log(value/lag(value)) to capture percentage price changes. Realized volatility (RV) was then calculated as the sum of squared log returns for each trading day. This daily RV was transformed by taking the square root of the variance, ensuring a more interpretable measure. Finally, rolling weekly and monthly realized volatilities were computed using sliding window functions, aggregating the last 5 and 22 trading days, respectively. The resulting ftse dataset integrates daily, weekly, and monthly volatility measures, serving as a robust foundation for forecasting models such as HAR-RV.

## **1. Load Processed Data**
```{r load_data, echo=FALSE}
# Load the cleaned FTSE dataset
ftse <- read_csv("ftse_rv_dwm2025.csv", show_col_types = FALSE)
# Calculate log returns for value
ftse %>% head(5)
```

## **2.Estimate the Corsi (2009) HAR model using a rolling moving window**
```{r HAR_model, echo=TRUE}
# Function to estimate the HAR model with a rolling window of 1000 observations
forecast_har <- function(newdata){

  fit <- lm(rv ~ 1 + rv_lag_1 + rv_weekly + rv_monthly, data = newdata)
  predict(fit, newdata[NROW(newdata), , drop = FALSE])
}

# Apply the function using a rolling window approach
har <- ftse %>% 
  mutate(har = slide_dbl(.x = ., 
                         ~ forecast_har(.x), 
                         .before = 999, 
                         .complete = TRUE) %>% lag(1)) %>% drop_na() %>% 
  mutate(residuals=rv-har )
```


## **2.1. Calculate Loss Functions - MAE and RMSE**
```{r mae_rmse, echo=TRUE}
loss_har <- har %>% 
  summarise(RSS = sum((residuals)^2, na.rm = TRUE),
            MAE = mean(abs(residuals), na.rm = TRUE), 
            RMSE = sqrt(mean((residuals)^2, na.rm = TRUE)),
            R_2 = 1 - sum((residuals)^2, na.rm = TRUE) / sum((rv - mean(rv, na.rm = TRUE))^2, na.rm = TRUE))
print(loss_har)
```

## **2.2. Visualizing the HAR Model Forecast**
```{r har_plot, echo=TRUE}
ggplot(har, aes(x = date)) +
  geom_line(aes(y = rv, color = "Realized Volatility (rv)"), linewidth = 1) +  
  geom_line(aes(y = har, color = "HAR Model Forecast"), linewidth = 1) +      
  labs(title = "Comparison of Realized Volatility and HAR Model Forecast",
       x = "Date", y = "Volatility",
       color = "Legend") + 
  theme_minimal() +
  theme(legend.position = "top")
```

## **3. Creating Training and Testing Sets**
```{r creating_training_testing, echo=TRUE }
# Split the series into training and testing sets
n <- nrow(ftse) # Total number of observations

# Determine the cutoff point (80% for training)
cutoff <- floor(0.8 * n) # Round down to the nearest integer

# Create train and test sets
train <- ftse[1:cutoff, ]      # First 80% for training
test <- ftse[(cutoff + 1):n, ] # Remaining 20% for testing

# Check sizes of train and test sets
cat("Training set:", nrow(train), "observations\n") 
cat("Testing set:", nrow(test), "observations\n")   
```

## **4.Estimate HAR model simple**
```{r hars_model, echo=TRUE}
# Function to estimate the HAR model simple
fit_har_s <- lm(rv ~ 1 + rv_lag_1 + rv_weekly + rv_monthly, data = train)

test_har_s <- predict(fit_har_s, test)

har_s <- test %>% 
  mutate(har_s = test_har_s %>% lag(1), residuals = test$rv-har_s) %>% 
  select(date, rv,har_s, residuals ) %>% drop_na()
```

**lag(1):** The purpose of applying lag(1) is to ensure that the projections (har_s) are shifted forward by one step, effectively excluding the first forecasted value. This adjustment is necessary because the model relies on past data to generate the next prediction. In the context of financial models, this prevents incorrect comparisons (known as look-ahead bias), ensuring that the forecast for time t is based solely on data available up to time t-1. This approach maintains the integrity of the predictive evaluation by aligning forecasts with the appropriate observed values.


## **4.1. HAR_s Loss Functions - MAE and RMSE****
```{r s_mae_rmse, echo=TRUE}
loss_har_s <- har_s %>% 
  summarise(RSS = sum((residuals)^2, na.rm = TRUE),
            MAE = mean(abs(residuals), na.rm = TRUE), 
            RMSE = sqrt(mean((residuals)^2, na.rm = TRUE)),
            R_2 = 1 - sum((residuals)^2, na.rm = TRUE) / sum((rv - mean(rv, na.rm = TRUE))^2, na.rm = TRUE))
print(loss_har_s)
```

## **4.2. Visualizing the HAR model simple Forecast**
```{r hars_plot, echo=TRUE}
ggplot(har_s, aes(x = date)) +
  geom_line(aes(y = rv, color = "Realized Volatility (rv)"), linewidth = 1) +  
  geom_line(aes(y = har_s, color = "HAR Model Forecast"), linewidth = 1) +      
  labs(title = "Comparison of Realized Volatility andHAR model simple",
       x = "Date", y = "Volatility",
       color = "Legend") + 
  theme_minimal() +
  theme(legend.position = "top")
```

## **5. ARIMA Model**
```{r arima, echo=TRUE}
# Fit ARIMA model on training data
arima_train <- auto.arima(train$rv)

# Display model summary
summary(arima_train)
```
## **5.1. ARIMA Model - Forecast and Plot**
```{r arima_plot, echo=TRUE}
# Generate forecasts for the test set
arima_forecast <- forecast(arima_train, h = length(test$rv))

# Plot the forecast vs actual test data
plot(arima_forecast, main = "ARIMA Forecast vs Test Data")
legend("topright", legend = c("ARIMA Forecast"),
       col = c("blue"), lty = 1, lwd = 2)

```


```{r arima_plot2, include=FALSE}
## **5.2. ARIMA Forecast vs Actual Test Data**

arima_forecast_values <- arima_forecast$mean  

# Create an index for the test set
test_time <- seq_along(test$rv)  

# Plot actual vs predicted values
plot(test_time, test$rv, type = "l", col = "red", lwd = 2,
     main = "ARIMA Forecast vs Test Data (Focused Comparison)",
     xlab = "Testing Period (Observations)", ylab = "Volatility",
     ylim = range(c(test$rv, arima_forecast_values)))

lines(test_time, arima_forecast_values, col = "blue", lwd = 2)  

legend("topright", legend = c("Actual Test Data", "ARIMA Forecast"),
       col = c("red", "blue"), lty = 1, lwd = 2)
```

```{r test_arima, include=FALSE}
## **Test ARIMA**
zx <- 1800
forecast_arima <- function(newdata) {
  #model <- Arima(newdata, order = c(5, 0, 0))  # Fit an ARIMA(5,0,0) model
  model <- auto.arima(newdata)
  return(forecast(model, h = 1)$mean)
}

forecast_arima(ftse$rv[1:zx])
ftse$rv[zx+1]
```

## **5.2. ARIMA Model - Moving Window Forecasting**
```{r arima_slide_dbl, echo=TRUE}
# Start overall execution time measurement
start_time <- Sys.time()

window <- 1000

# Function to fit ARIMA and forecast one step ahead
forecast_arima <- function(newdata) {
  model <- auto.arima(newdata)
  return(forecast(model, h = 1)$mean)
}

# Apply the function using a rolling window approach with slide_dbl
arima_forecast <- ftse %>%
  mutate(arima_pred = slide_dbl(.x = rv, 
                                .f = ~ forecast_arima(.x), 
                                .before = (window-1), 
                                .complete = TRUE) %>% lag(1)) %>% 
  drop_na() %>% mutate(residuals = rv - arima_pred)

# End execution time measurement
end_time <- Sys.time()
execution_time <- end_time - start_time

# Print the total execution time
print(execution_time)
```

## **5.3. ARIMA Loss Functions - MAE and RMSE****
```{r loss_arima, echo=TRUE}
loss_arima <- arima_forecast %>% 
  summarise(RSS = sum((residuals)^2, na.rm = TRUE),
            MAE = mean(abs(residuals), na.rm = TRUE), 
            RMSE = sqrt(mean((residuals)^2, na.rm = TRUE)),
            R_2 = 1 - sum((residuals)^2, na.rm = TRUE) / sum((rv - mean(rv, na.rm = TRUE))^2, na.rm = TRUE))
print(loss_arima)
```

## **5.4. ARIMA Moving Window Forecasting**
```{r arima_slide_plot, echo=TRUE}
# Plot ARIMA Moving Window Forecasting
library(ggplot2)
ggplot(arima_forecast, aes(x = date)) +
  geom_line(aes(y = rv, color = "Actual Volatility")) +
  geom_line(aes(y = arima_pred, color = "Rolling ARIMA Forecast")) +
  labs(title = "ARIMA Model with Rolling Window Forecasting",
       x = "Date", y = "Volatility",
       color = "Legend") +
  theme_minimal()
```


```{r estimate_d, echo=TRUE, include=FALSE}
## **6. AFIRMA Model****

library(fracdiff)

# Estimate d value using fracdiff
model_d <- fracdiff(ftse$rv, nar = 5, nma = 0)
d_value <- model_d$d  

# Perform fractional differencing on the training data
train_diff <- diffseries(train$rv, d_value)  

# Fit the ARIMA(5,d,0) model using the differenced series
afirma_model <- Arima(train_diff, order = c(5, 0, 0))

# Display the model summary
summary(afirma_model)
```


```{r arfima_plot, include=FALSE}
## **6.1. ARFIMA Forecasting**
# Forecast using the AFIRMA model on test data
afirma_forecast <- forecast(afirma_model, h = nrow(test))

# Extract forecasted values
afirma_forecast_values <- afirma_forecast$mean  

# Create an index for the test set
test_time <- seq_along(test$rv)  

# Plot actual vs forecasted values
plot(test_time, test$rv, type = "l", col = "red", lwd = 2,
     main = "AFIRMA(5,d,0) Forecast vs Test Data",
     xlab = "Testing Period (Observations)", ylab = "Volatility",
     ylim = range(c(test$rv, afirma_forecast_values)))

lines(test_time, afirma_forecast_values, col = "blue", lwd = 2)  

legend("topright", legend = c("Actual Test Data", "AFIRMA Forecast"),
       col = c("red", "blue"), lty = 1, lwd = 2)

```


```{r test_arfima, include=FALSE}
## **Test ARFIMA**

zx <- 1800
test <- ftse$rv[1:zx]

# Step 4: Function to fit ARFIMA(5,d,0) and forecast one step ahead
forecast_afirma <- function(newdata) {
  diff <- diffseries(newdata, 0.4)
  model <- Arima(diff, order = c(5, 0, 0))
  forecast_diff <- forecast(model, h = 1)$mean
  # Reintegrate the forecast to the original scale
  reintegrated_forecast <- forecast_diff + newdata[length(newdata )]
  return(reintegrated_forecast)
}

forecast_afirma(test)
ftse$rv[zx+1]
```



```{r arima_moving_window, include=FALSE}
## **6.2. ARFIMA Moving Window Forecasting**

## **ARFIMA(5,d,0) Model Implementation for FTSE Dataset**

# Step 1: Estimate the fractional differencing parameter 'd' on the whole sample
model_d <- fracdiff(ftse$rv, nar = 5, nma = 0)
d_value <- model_d$d  

# Step 2: Perform fractional differencing on the whole dataset
ftse$rv_diff <- diffseries(ftse$rv, d_value)

# Step 3: Fit a single ARIMA(5,0,0) model on the fractionally differenced data
arfima_model <- Arima(ftse$rv_diff, order = c(5, 0, 0))

# Step 4: Forecast on the differenced scale and reintegrate to the original scale
forecasts <- forecast(arfima_model, h = length(ftse$rv))$mean  # Generate all forecasts
reintegrated_forecasts <- forecasts + lag(ftse$rv)  # Reintegrate to original scale

# Step 5: Combine results into a data frame for analysis
afirma <- data.frame(
  observation = 1:length(ftse$rv),
  rv = ftse$rv,
  predicted = reintegrated_forecasts
) %>%
  mutate(residuals = rv - predicted) %>%
  drop_na()
```


```{r arima_moving_window_plot, include=FALSE}
## **6.3. ARFIMA Moving Window Plot**
library(ggplot2)
ggplot(afirma, aes(x = observation)) +
  geom_line(aes(y = rv, color = "Actual Values"), size = 1) +
  geom_line(aes(y = predicted, color = "AFIRMA Forecast"), size = 1) +
  labs(title = "AFIRMA(5,d,0) Moving Window Forecast with Loop",
       x = "Observation",
       y = "Volatility") +
  theme_minimal()
```


```{r loss_afirma, include=FALSE}
## **6.4. ARFIMA Loss Functions**
loss_arfima <- afirma %>%
  summarise(
    RSS = sum((residuals)^2, na.rm = TRUE),
    MAE = mean(abs(residuals), na.rm = TRUE),
    RMSE = sqrt(mean((residuals)^2, na.rm = TRUE)),
    R_2 = 1 - sum((residuals)^2, na.rm = TRUE) / sum((rv - mean(rv, na.rm = TRUE))^2, na.rm = TRUE)  )
print(loss_arfima)
```

## **6. Moving Averages**
```{r sma_test, include=FALSE}
## ** SMA**
len <- length(ftse$rv)
smas <- c(1:100)
MAE <- numeric(length(smas))  
RMSE <- numeric(length(smas))
R_2 <- numeric(length(smas))

for(i in smas){
  loop <- rep(NA, len)
  for(j in (i:len)){
    loop[j] <- sum( ftse$rv[(j-(i-1)):j] )/i
  }
  residual <- ftse$rv-lag(loop,1)
  MAE[i] <- mean(abs(residual), na.rm = TRUE)
  RMSE[i] <-  sqrt(mean((residual)^2, na.rm = TRUE))
  R_2[i] = 1 - sum((residual)^2, na.rm = TRUE) / sum((ftse$rv - mean(ftse$rv, na.rm = TRUE))^2, na.rm = TRUE)  
}

start <- 2
sma_results <- data.frame(
 Period = start:length(smas),
 MAE = MAE[start:length(smas)],
 RMSE = RMSE[start:length(smas)],
 R_2 = R_2[start:length(smas)])

best_sma <- sma_results[which.min(sma_results$MAE), ]
print(sma_results)
print(best_sma)
```


## **6.1. Optimizing SMA and EMA Periods Based on MAE and RMSE**
```{r smas_ema, echo=TRUE}
# Load the TTR library for SMA and EMA calculations
library(TTR)

# Define the length of the data and maximum window size to test
len <- length(ftse$rv)
max_period <- 100  # Test SMA and EMA for window sizes from 1 to 100

# Create a data frame to store MAE and RMSE for each period
results <- data.frame(Period = 1:max_period, SMA_MAE = NA, SMA_RMSE = NA, EMA_MAE = NA, EMA_RMSE = NA)

# Loop through each period to calculate SMA and EMA metrics
for (n in 1:max_period) {
  # Calculate SMA and EMA for the current period
  sma <- SMA(ftse$rv, n = n)  # Simple Moving Average
  ema <- EMA(ftse$rv, n = n)  # Exponential Moving Average
  
  # Calculate residuals with lag(1) to avoid look-ahead bias
  residual_sma <- ftse$rv - lag(sma, 1)
  residual_ema <- ftse$rv - lag(ema, 1)
  
  # Calculate Mean Absolute Error (MAE) and Root Mean Square Error (RMSE)
  results[n, "SMA_MAE"] <- mean(abs(residual_sma), na.rm = TRUE)
  results[n, "SMA_RMSE"] <- sqrt(mean(residual_sma^2, na.rm = TRUE))
  results[n, "EMA_MAE"] <- mean(abs(residual_ema), na.rm = TRUE)
  results[n, "EMA_RMSE"] <- sqrt(mean(residual_ema^2, na.rm = TRUE))
}

# Identify the best period (minimum MAE) for SMA and EMA
best_sma <- results[which.min(results$SMA_MAE), "Period"]
best_ema <- results[which.min(results$EMA_MAE), "Period"]

# Print the best periods for SMA and EMA
print(paste("Best SMA Period: ", best_sma, " periods"))
print(paste("Best EMA Period: ", best_ema, " periods"))
```

## **6.2. Rolling oving window SMA - Simple Moving Average**
```{r best_smas, echo=TRUE}

# Function to calculate the best SMA in a rolling window of 1000 observations
calculate_best_sma <- function(data_window, max_period = 100) {
  errors <- sapply(1:max_period, function(n) {
    sma_values <- SMA(data_window, n = n)
    residuals <- data_window - lag(sma_values, 1)  # Avoid look-ahead bias
    mean(abs(residuals), na.rm = TRUE)  # Calculate MAE
  })
  which.min(errors)
}

# SMA rolling moving window
sma <- data.frame(rv = ftse$rv) %>%
  mutate(sma = slide_dbl(.x = seq_along(rv), 
                         ~ {
                           window <- rv[.x] 
                           if (length(window) < 1000) return(NA)  
                           best_period <- calculate_best_sma(window, max_period = 100)
                           sma_values <- SMA(window, n = best_period)
                           tail(sma_values, 1)  
                         },
                         .before = 999, 
                         .complete = TRUE) %>% lag(1)) %>%  drop_na() %>%
  mutate(residuals = rv - sma, obs = row_number())

# Calculate MAE, RMSE, and R^2 for SMA
sma_mae <- mean(abs(sma$residuals), na.rm = TRUE)
sma_rmse <- sqrt(mean(sma$residuals^2, na.rm = TRUE))
sma_r2 <- 1 - sum(sma$residuals^2, na.rm = TRUE) / sum((sma$rv - mean(sma$rv, na.rm = TRUE))^2, na.rm = TRUE)

# Plot SMA vs Actual Values
ggplot(sma, aes(x = obs)) +
  geom_line(aes(y = rv, color = "Actual Values"), size = 1) +
  geom_line(aes(y = sma, color = "SMA Forecast"), size = 1) +
  labs(title = paste("SMA vs Actual Volatility", sep = ""),
       x = "Observation",
       y = "Volatility") +
  theme_minimal()
```

## **6.3. Best EMA - Exponential Moving Average**
```{r best_ema, echo=TRUE}

# Function to calculate the best EMA in a rolling window of 1000 observations
calculate_best_ema <- function(data_window, max_period = 100) {
  errors <- sapply(1:max_period, function(n) {
    ema_values <- EMA(data_window, n = n)
    residuals <- data_window - lag(ema_values, 1)  # Avoid look-ahead bias
    mean(abs(residuals), na.rm = TRUE)  # Calculate MAE
  })
  which.min(errors)
}

# EMA rolling moving window
ema <- data.frame(rv = ftse$rv) %>%
  mutate(ema = slide_dbl(.x = seq_along(rv), 
                         ~ {
                           window <- rv[.x] 
                           if (length(window) < 1000) return(NA)  
                           best_period <- calculate_best_ema(window, max_period = 100)
                           ema_values <- EMA(window, n = best_period)
                           tail(ema_values, 1)  
                         },
                         .before = 999, 
                         .complete = TRUE) %>% lag(1)) %>%  drop_na() %>%
  mutate(residuals = rv - ema, obs = row_number())

# Calculate MAE, RMSE, and R^2 for EMA
ema_mae <- mean(abs(ema$residuals), na.rm = TRUE)
ema_rmse <- sqrt(mean(ema$residuals^2, na.rm = TRUE))
ema_r2 <- 1 - sum(ema$residuals^2, na.rm = TRUE) / sum((ema$rv - mean(ema$rv, na.rm = TRUE))^2, na.rm = TRUE)

# Plot EMA vs Actual Values
ggplot(ema, aes(x = obs)) +
  geom_line(aes(y = rv, color = "Actual Values"), size = 1) +
  geom_line(aes(y = ema, color = "EMA Forecast"), size = 1) +
  labs(title = paste("EMA (", best_ema, "-Period) vs Actual Volatility", sep = ""),
       x = "Observation",
       y = "Volatility") +
  theme_minimal()
```

## **6.4. Best EWMA - Exponentially Weighted Moving Average - lambda**
```{r best_ewma_lambda, echo=TRUE}
# Define a range of ratio values to test
ratios <- seq(0.01, 0.99, by = 0.01)  # Test values from 0.01 to 0.99
mae_results <- numeric(length(ratios))  # Store MAE results

# Loop to test different ratios
for (i in seq_along(ratios)) {
  ewma_test <- EMA(ftse$rv, ratio = ratios[i])  # Compute EWMA for each ratio
  residuals <- ftse$rv - lag(ewma_test, 1)  # Align forecast with actual values
  mae_results[i] <- mean(abs(residuals), na.rm = TRUE)  # Compute MAE
}

# Find the best ratio (minimum MAE)
best_ratio <- ratios[which.min(mae_results)]
print(paste("Best lambda based on MAE:", best_ratio))
```


## **6.5. Best EWMA lambda**
```{r best_ewma, echo=TRUE}
# Function to calculate the best EWMA ratio in a rolling window
calculate_best_ewma <- function(data_window, ratio_range = seq(0.01, 0.99, by = 0.01)) {
  errors <- sapply(ratio_range, function(ratio) {
    ewma_values <- EMA(data_window, ratio = ratio)  # Compute EWMA with the current ratio
    residuals <- data_window - lag(ewma_values, 1)  # Avoid look-ahead bias
    mean(abs(residuals), na.rm = TRUE)  # Calculate MAE
  })
  # Return the ratio that minimizes the MAE
  ratio_range[which.min(errors)]
}

# EWMA rolling moving window
ewma <- data.frame(rv = ftse$rv) %>%
  mutate(ewma = slide_dbl(.x = seq_along(rv), 
                          ~ {
                            window <- rv[.x]  
                            if (length(window) < 1000) return(NA) 
                            best_ratio <- calculate_best_ewma(window, ratio_range = seq(0.01, 0.99, by = 0.01))
                            ewma_values <- EMA(window, ratio = best_ratio)
                            tail(ewma_values, 1)  
                          },
                          .before = 999, 
                          .complete = TRUE) %>% lag(1)) %>%  
  drop_na() %>%   mutate(residuals = rv - ewma, obs = row_number())

# Calculate MAE, RMSE, and R^2 for EWMA
ewma_mae <- mean(abs(ewma$residuals), na.rm = TRUE)
ewma_rmse <- sqrt(mean(ewma$residuals^2, na.rm = TRUE))
ewma_r2 <- 1 - sum(ewma$residuals^2, na.rm = TRUE) / sum((ewma$rv - mean(ewma$rv, na.rm = TRUE))^2, na.rm = TRUE)

# Plot EWMA vs Actual Values
ggplot(ewma, aes(x = obs)) +
  geom_line(aes(y = rv, color = "Actual Values"), size = 1) +
  geom_line(aes(y = ewma, color = "EWMA Forecast"), size = 1) +
  labs(title = paste("EWMA with Optimized Ratio (", round(best_ratio, 3), ") vs Actual Volatility", sep = ""),
       x = "Observation",
       y = "Volatility") +
  theme_minimal()
```
## **EMA vs EWMA**

**EMA Explanation:** 
The Exponential Moving Average (EMA) is a weighted moving average that gives greater importance to recent data points. Unlike the Simple Moving Average (SMA), which assigns equal weights to all observations, the EMA applies an exponentially decreasing weight to older data. The degree of smoothing is controlled by the period (n), which determines the weight given to past values.

The traditional formula for **EMA** is given by:

$$
EMA_t = \alpha \cdot rv_t + (1 - \alpha) \cdot EMA_{t-1}
$$

Alpha is the smoothing factor, defined as:

$$
\alpha = \frac{2}{n+1}
$$
Where n is the number of periods for the moving average.


**EWMA Explanation:**
The Exponentially Weighted Moving Average (EWMA) is a specific type of EMA where the user explicitly defines the smoothing factor, known as the "ratio" (λ). Unlike EMA, which calculates the ratio based on the number of periods (n), EWMA allows for direct control over the decay rate of past observations. This makes EWMA more suitable for financial applications, such as volatility modeling, where recent price movements should have a stronger impact on future predictions.

The formula for **EWMA** is similar to EMA but uses a specific decay factor lambda:

$$
EWMA_t = \lambda \cdot rv_t + (1 - \lambda) \cdot EWMA_{t-1}
$$

**Conclusion EMA vs EWMA:**
In this implementation, we optimize the EWMA by selecting the best smoothing ratio using the Mean Absolute Error (MAE) criterion. The optimized ratio ensures that the EWMA forecast closely follows the actual data while maintaining a balance between responsiveness and stability. By avoiding a predefined period (n), this approach provides a more flexible and data-driven method for volatility modeling.


| **Model** | **Weight for \( rv_t \)** | **Weight for \( EWMA_{t-1} \)** |
|-----------|---------------------------|---------------------------------|
| **EMA**   | \( \frac{2}{n+1} \)       | \( 1 - \frac{2}{n+1} \)        |
| **EWMA**  | \( \lambda \)             | \( 1 - \lambda \)              |


## **7. Autoregressive conditionally heteroscedastic (ARCH) models**
```{r best_arch, echo=TRUE}
# Load required library
library(rugarch)

# Define a function to find the best ARCH model
find_best_arch <- function(data, max_q) {
    results <- data.frame(Order = 1:max_q, AIC = NA, BIC = NA)
    
    for (q in 1:max_q) {
        spec <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(q, 0)), 
                           mean.model = list(armaOrder = c(0, 0), include.mean = FALSE))
        fit <- ugarchfit(spec, data = data, solver = "hybrid")
        results[q, "AIC"] <- infocriteria(fit)[1]
        results[q, "BIC"] <- infocriteria(fit)[2]
    }
    
    return(results)
}


# Find the best ARCH model (max_q = 10)
max_q <- 10
arch_results <- find_best_arch(ftse$rv, max_q)

# Identify the best order based on AIC
best_arch_order <- which.min(arch_results$AIC)
print(paste("Best ARCH(q) model order based on AIC:", best_arch_order))
print(arch_results)

```

## **AIC vs BIV**

The Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) are commonly used methods for model selection in statistical analysis. Both aim to identify the model that provides the best balance between goodness of fit and complexity. The AIC is calculated as −2⋅log-likelihood+2⋅k, wher ek is the number of parameters in the model. It rewards goodness of fit but penalizes models with more parameters, favoring lower AIC values. The BIC, on the other hand, is stricter and includes the sample size (n) in its penalty for complexity, calculated as −2⋅log-likelihood+log(n)⋅k. Lower BIC values also indicate better models, but the stricter penalty for additional parameters means BIC often selects simpler models compared to AIC, particularly in large datasets. When comparing models, the one with the lowest AIC or BIC is considered better. However, if the two criteria suggest different models, the choice often depends on the context, with AIC favoring more complex models and BIC emphasizing simplicity.


## **7.1. Best ARCH Model**
```{r arch, echo=TRUE}
# Define the best ARCH model
best_q <- best_arch_order  # From previous step
spec <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(best_q, 0)), 
                   mean.model = list(armaOrder = c(0, 0), include.mean = FALSE))

# Fit the ARCH model
fit <- ugarchfit(spec, data = ftse$rv, solver = "hybrid")

# Extract fitted values (conditional variances)
fitted_volatility <- sigma(fit)

# Prepare data for plotting
arch_data <- data.frame(observation = 1:length(ftse$rv), 
                        rv = ftse$rv, 
                        fitted_volatility = lag(fitted_volatility,1)) %>% drop_na()

# Plot ARCH model vs Actual Values
library(ggplot2)
ggplot(arch_data, aes(x = observation)) +
  geom_line(aes(y = rv, color = "Actual Volatility"), size = 1) +
  geom_line(aes(y = fitted_volatility, color = "ARCH Fitted Volatility"), size = 1) +
  labs(title = paste("ARCH(", best_q, ") Model vs Actual Volatility", sep = ""),
       x = "Observation",
       y = "Volatility") +
  theme_minimal()



```

## **7.2. ARCH Loss Functions**
```{r loss_arch, echo=TRUE}
# Calculate residuals for ARCH
arch_data <- arch_data %>%
  mutate(residuals = rv - fitted_volatility)

# Calculate loss functions for ARCH
loss_arch <- arch_data %>%
  summarise(
    MAE = mean(abs(residuals), na.rm = TRUE),
    RMSE = sqrt(mean((residuals)^2, na.rm = TRUE)),
    R_2 = 1 - sum((residuals)^2, na.rm = TRUE) / sum((rv - mean(rv, na.rm = TRUE))^2, na.rm = TRUE)
  )

# Print the loss functions for ARCH
print(loss_arch)

```

## **8. Generalised ARCH (GARCH) models**
```{r best_garch, echo=TRUE}
# Define a function to find the best GARCH model
find_best_garch <- function(data, max_p, max_q) {
    results <- data.frame(P = integer(0), Q = integer(0), AIC = numeric(0), BIC = numeric(0))
    
    for (p in 1:max_p) {
        for (q in 1:max_q) {
            spec <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(p, q)), 
                               mean.model = list(armaOrder = c(0, 0), include.mean = FALSE))
            fit <- tryCatch(ugarchfit(spec, data = data, solver = "hybrid"), error = function(e) NULL)
            if (!is.null(fit)) {
                results <- rbind(results, data.frame(P = p, Q = q, 
                                                     AIC = infocriteria(fit)[1], 
                                                     BIC = infocriteria(fit)[2]))
            }
        }
    }
    return(results)
}

# Find the best GARCH model (max_p = 5, max_q = 5)
max_p <- 5
max_q <- 5
garch_results <- find_best_garch(ftse$rv, max_p, max_q)

# Identify the best order based on AIC
best_garch_order <- garch_results[which.min(garch_results$AIC), c("P", "Q")]
print(paste("Best GARCH(p, q) model order based on AIC: P =", best_garch_order$P, ", Q =", best_garch_order$Q))
print(garch_results)

```
## **8.1. Best ARCH Model**
```{r garch, echo=TRUE}
# Define the best GARCH model
best_p <- best_garch_order$P  # From previous step
best_q <- best_garch_order$Q  # From previous step
spec <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(best_p, best_q)), 
                   mean.model = list(armaOrder = c(0, 0), include.mean = FALSE))

# Fit the GARCH model
fit <- ugarchfit(spec, data = ftse$rv, solver = "hybrid")

# Extract fitted values (conditional variances)
fitted_volatility <- sigma(fit)

# Prepare data for plotting
garch_data <- data.frame(observation = 1:length(ftse$rv), 
                         rv = ftse$rv, 
                         fitted_volatility = lag(fitted_volatility,1)) %>% drop_na()

# Plot GARCH model vs Actual Values
library(ggplot2)
ggplot(garch_data, aes(x = observation)) +
  geom_line(aes(y = rv, color = "Actual Volatility"), size = 1) +
  geom_line(aes(y = fitted_volatility, color = "GARCH Fitted Volatility"), size = 1) +
  labs(title = paste("GARCH(", best_p, ",", best_q, ") Model vs Actual Volatility", sep = ""),
       x = "Observation",
       y = "Volatility") +
  theme_minimal()

```
## **8.2. GARCH Loss Functions**
```{r loss_garch, echo=TRUE}
# Calculate residuals for GARCH
garch_data <- garch_data %>%
  mutate(residuals = rv - fitted_volatility)

# Calculate loss functions for GARCH
loss_garch <- garch_data %>%
  summarise(
    MAE = mean(abs(residuals), na.rm = TRUE),
    RMSE = sqrt(mean((residuals)^2, na.rm = TRUE)),
    R_2 = 1 - sum((residuals)^2, na.rm = TRUE) / sum((rv - mean(rv, na.rm = TRUE))^2, na.rm = TRUE)
  )

# Print the loss functions for GARCH
print(loss_garch)

```
## **9. Loss Functions Comparison**
```{r create_dataframe, echo=FALSE, include=TRUE}
library(formattable)

# Create the data frame with models and metrics
error_metrics <- data.frame(
  Models = c("HAR", "HAR_S", "ARIMA", "SMA", "EMA", "EWMA", "ARCH*", "GARCH*"),
  MAE = c(loss_har$MAE, loss_har_s$MAE, loss_arima$MAE, sma_mae, ema_mae, ewma_mae, loss_arch$MAE, loss_garch$MAE),
  RMSE = c(loss_har$RMSE, loss_har_s$RMSE, loss_arima$RMSE, sma_rmse, ema_rmse, ewma_rmse, loss_arch$RMSE, loss_garch$RMSE),
  R_2 = c(loss_har$R_2, loss_har_s$R_2, loss_arima$R_2, sma_r2, ema_r2, ewma_r2, loss_arch$R_2, loss_garch$R_2)
)

# Highlight the best values in the table
formatted_metrics <- formattable(
  error_metrics,
  list(
    MAE = formatter("span", style = x ~ style(color = ifelse(x == min(error_metrics$MAE), "green", "black"))),
    RMSE = formatter("span", style = x ~ style(color = ifelse(x == min(error_metrics$RMSE), "green", "black"))),
    R_2 = formatter("span", style = x ~ style(color = ifelse(x == max(error_metrics$R_2), "blue", "black")))
  )
)

# Print the formatted table
formatted_metrics

# Add explanation for the asterisk (*)
cat("\n* The parameters for 'ARCH*' and 'GARCH*' models were estimated using the entire dataset (in-sample), which can lead to overly optimistic results. In-sample evaluation means the model is trained and tested on the same data, making it prone to overfitting and using information from the future that would not be available in real-world forecasting.")
```

## **Conclusion for the Volatility Models Document**

This study aimed to explore various volatility models applied to the FTSE-100 index, including HAR-type models, ARIMA, SMA, EMA, EWMA, and ARCH/GARCH. Each model was tested to evaluate its ability to forecast realized volatility, with the results measured using key performance metrics such as MAE, RMSE, and R².

Among the models examined, the EMA (Exponential Moving Average) demonstrated the best overall performance, achieving the lowest MAE and RMSE while attaining the highest R² value. This highlights its strength as a forecasting tool for capturing the dynamics of financial market volatility.

It is essential to note that this work was conducted as a portfolio project to demonstrate the application of volatility forecasting models and to showcase expertise in their implementation. The results are not intended to serve as conclusive evidence that EMA is universally the best model for forecasting volatility. The analysis does not account for statistical rigor often required in scientific research, and the models were not validated under diverse financial conditions or datasets.

In summary, while the findings provide valuable insights into the comparative performance of various volatility models, this document should be interpreted as a demonstration of applied modeling techniques rather than a definitive scientific study.



```{r export_pdf, include=FALSE, echo=TRUE, eval=FALSE}
## **Exporting Code to PDF**
# Knit document to PDF with optimizations

#rmarkdown::render(
  #input = "volatility_models.Rmd", 
  #output_format = "pdf_document", 
  #quiet = TRUE)
```




